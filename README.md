# AdventureWorks Sales Analysis | End-to-End Azure Data Pipeline

## Overview
This project demonstrates a comprehensive data engineering pipeline built on Azure Cloud. The pipeline covers the entire lifecycle of data—from ingestion and transformation to loading and reporting. The source data is the AdventureWorksLT2022 database, which is restored from a .bak file, and the final output is a set of interactive reports built in Power BI.

## Project Goals
1. **Connect On-Premise SQL Server with Azure Cloud**: Establish a secure connection between an on-premise SQL Server and Azure using Microsoft Integration Runtime.
2. **Ingest Data into Azure Data Lake**: Migrate tables from the on-premise SQL Server into Azure Data Lake Storage Gen2.
3. **Transform Data Using Azure Databricks**: Clean and process the ingested data through transformations in Databricks, moving it from a raw Bronze layer to a refined Gold layer.
4. **Load Data into Azure Synapse Analytics**: Load the transformed and cleaned data into Azure Synapse Analytics for efficient querying.
5. **Visualize Data with Power BI**: Create interactive dashboards and reports in Power BI connected to the Azure Synapse database.
6. **Implement Security and Monitoring**: Utilize Azure Active Directory (AAD) and Azure Key Vault to secure and monitor the data pipeline.

## Technologies Used
- **Data Source**: SQL Server
- **Orchestration**: Azure Data Factory
- **Storage**: Azure Data Lake Storage Gen2, Azure Synapse Analytics
- **Transformation**: Azure Databricks (PySpark)
- **Reporting**: Microsoft Power BI
- **Security**: Azure Active Directory, Azure Key Vault

## Project Architecture
The project architecture consists of several stages:

![AdventureWorks Architecture](d:\Data Engineering\powerbi\architecture.png)

### 1. Data Ingestion
- **Restore AdventureWorksLT2017 Database**: The initial step involves restoring the AdventureWorksLT2017 database from a .bak file.
- **Set Up Integration Runtime**: Establish a connection between the on-premise SQL Server and Azure using the Microsoft Integration Runtime.
- **Copy Data to Azure Data Lake Storage Gen2**: Use Azure Data Factory (ADF) to create a pipeline that copies data from the on-premise SQL Server to the "bronze" directory in Azure Data Lake Storage Gen2 in Parquet format.

### 2. Data Transformation
- **Mount ADLS in Azure Databricks**: Mount the Azure Data Lake Storage Gen2 to Azure Databricks to access the raw data.
- **Bronze to Silver Transformation**: Perform initial data cleaning and type transformations, moving the processed data to the "silver" directory.
- **Silver to Gold Transformation**: Apply consistent naming conventions to the data attributes and move the final transformed data to the "gold" directory in Delta format. This data is now ready for reporting.

### 3. Data Loading
- **Load Data into Azure Synapse Analytics**: Run a pipeline in Azure Synapse that retrieves the table names from the Gold folder in ADLS and creates or updates views in an Azure SQL Database using stored procedures.

### 4. Data Reporting
- **Create Power BI Dashboards**: Connect Power BI to the Azure Synapse database using DirectQuery to create interactive dashboards that reflect the latest data from the pipeline.
- **Key Insights**: 
  - **Revenue by Product Category**: Touring Bikes lead with 32% of total revenue, followed by Road Bikes (26%) and Mountain Bikes (24%).
  - **Sales by Country**: The UK tops the sales chart with 278 sales and $572,000 in revenue, followed by the USA with 264 sales and $383,810 in revenue.
  - **Revenue by Gender**: 81% of revenue is generated by male customers, while female customers contribute 19%.

### 5. Security and Monitoring
- **Use Azure Active Directory (AAD)**: Secure the data pipeline and manage access through Azure Active Directory.
- **Utilize Azure Key Vault**: Manage secrets and monitor the data pipeline securely using Azure Key Vault.



## Dataset
The dataset used in this project is the AdventureWorksLT2017 database—a lightweight, OLTP sample database provided by Microsoft to demonstrate SQL Server capabilities. The database supports a manufacturing company named Adventure Works Cycles and includes a pared-down version of their transactional data.

## Implementation Steps
### Part 1: Data Ingestion
1. **Restore AdventureWorksLT2017 Database**: Restore the database from a .bak file on the on-premise SQL Server.
2. **Set Up Integration Runtime**: Install and configure the Microsoft Integration Runtime to connect the on-premise SQL Server to Azure.
3. **Create ADF Copy Pipeline**: Set up a pipeline in Azure Data Factory to copy data from the on-premise SQL Server to Azure Data Lake Storage Gen2, storing it in Parquet format.

### Part 2: Data Transformation
1. **Mount ADLS in Databricks**: Mount the Azure Data Lake Storage to Databricks for data access.
2. **Bronze to Silver Transformation**: Clean the data and apply attribute type changes, saving the output to the Silver folder.
3. **Silver to Gold Transformation**: Apply consistent naming conventions and save the final data in the Gold folder in Delta format.

### Part 3: Data Loading
1. **Load Data into Synapse**: Use Azure Synapse Analytics to efficiently load and manage the cleaned data.
2. **Execute Stored Procedure**: Run a stored procedure in Azure Synapse to create or update views in the Azure SQL Database.

### Part 4: Data Reporting
1. **Connect Power BI to Synapse**: Set up a DirectQuery connection between Power BI and Azure Synapse to create real-time, interactive dashboards.
2. **Build Dashboards**: Create insightful visualizations to analyze sales data by product category, country, and customer demographics.

## Conclusion
This project showcases an end-to-end data engineering pipeline on Azure, from data ingestion and transformation to loading and reporting. It also highlights the importance of security and monitoring using Azure's native tools. The final product is a set of dynamic Power BI dashboards that provide valuable business insights.

